\documentclass[10pt,landscape,a4paper]{cheatsheet}
\usepackage[ngerman]{babel}
\usepackage{float}
\usepackage{xtab}
\title{CS4414 Cheat-Sheet} 
\author{Gurpreet Singh\\\texttt{gsingh95@uwo.ca}}
\date{\today}

\newcommand{\Samp}{\mathcal{S}}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\diff}{\mathrm{\,d}}
\newcommand{\T}{\mathsf{T}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\w}{\mathbf{w}}
\renewcommand{\T}{\mathsf{T}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\y}{\mathbf{y}}


\begin{document}
  \maketitle
  \section{Data Preparation}

  \subsection{Data Cleaning}
  Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records
  from a record set, table, or database and refers to identifying incorrect parts of the data and then deleting the dirty or coarse data.

  \section{Statistics}

  \subsection{Definitions}
  \texttt{BoxPlot}: Extended lines mean Min and Max. Box is third quartile, median and first quartile top to bottom. Suspected outliers are empty dots, outliers are solid dots\newline

  \texttt{Histogram}: Same as a barchart but groups numbers into ranges on the x-axis\newline

  \texttt{Independence}: Two random variables X and Y are independent if Any conditional distribution of $X$ is the same as the marginal distribution of $X$ and knowing about $Y$ provides no information about $X$.\newline

  \texttt{Marginal Distribution}: gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. The sum of the columns.\newline

  \texttt{Conditional Distribution}: shows the probability that a randomly selected item in a sub-population has a characteristic you're interested in. Moving the column (scanning)\newline 
 
  \texttt{Joint Distribution}: $X$ and $Y$ have a joint distribution if their realizations come together as a pair. $(X,Y)$ is a random vector, and realizations may be written $(x_1,y_1), (x_2,y_2), ...$, or $\langle x_1, y_1 \rangle, \langle x_2, y_2 \rangle, ...$ \newline

  \texttt{Sample Mean}: Given a dataset (collection of realizations) $x_1, x_2, ..., x_n$ of $X$, the sample mean is:
   $$ \bar{x}_n = \frac{1}{n} \sum_i x_i $$
   Given a dataset, $\bar x_n$ is a fixed number. 
   It is usually a good estimate of the expected value of a random variable $X$ with an unknown distribution.\newline

  \texttt{Sample space} $\Samp$ is the set of all possible events we might observe. Depends on context.\\
    - Coin flips: $\Samp = \{ h, t \}$\\
    - Eruption times: $\Samp = \Re^{\ge 0}$\\
    - (Eruption times, Eruption waits): $\Samp = \Re^{\ge 0} \times \Re^{\ge 0}$ \newline

  \texttt{An event} is a subset of the sample space.\\
    - Observe heads: $\{ h \}$\\
    - Observe eruption for 2 minutes: $\{ 2.0 \}$\\
    - Observe eruption with length between 1 and 2 minutes and wait between 50 and 70 minutes: $[1,2] \times [50,70]$.

  \subsection{Random Variables}

  \texttt{Random variable} is a mapping from the event space to a number (or vector)\newline

  \texttt{Discrete random variable} take values from a countable set\newline

  \texttt{Continuous random variable} take values in intervals of $\Re$\\For a continuous r.v.\ $X$, $\Pr(X = x) = 0$ for all $x$. There is no probability mass function.

  \subsection{Probability Mass Function} 
  For a discrete $X$, $p_{X}(x)$ gives $\Pr(X = x)$.\\
  Requirement: $\sum_{x \in \mathcal{X}} p_{X}(x) = 1$.\\
  Note that the sum can have an infinite number of terms.\\
  - Only works on discrete values\\
  - Sum of probabilities in set needs to be 1

  \subsection{Cumulative Distribution function} 
  For a discrete $X$, $P_{X}(x)$ gives $\Pr(X \le x)$.\\
   Requirements:\\
    - $P$ is nondecreasing\\
    - $\sup_{x \in \mathcal{X}} P_{X}(x) = 1$

   Note:
    - $P_X(b) = \sum_{x \le b} p_X(x)$\\
    - $\Pr(a < X \le b) = P_X(b) - P_X(a)$

   \subsection{Probability Density Function}
   For continuous $X$, $\Pr(X = x) = 0$ and PMF does not exist. However, we define the Probability Density Function $f_X$:\\
    - $\Pr(a \le X \le b) = \int_{a}^{b} f_X(x) \diff x$\\
   Requirement:\\
    - $\forall x \;f_X(x) > 0$, $\int_{-\infty}^\infty f_X(x) \diff x = 1$\\
    - Requires a range, doesnt work with discrete values

   \subsection{Cumulative Distribution Function}
   For a continuous $X$, $F_{X}(x)$ gives $\Pr(X \le x) = \Pr(X \in (-\infty,x])$.\\
   Requirements: \\
    - $F$ is nondecreasing\\
    - $\sup_{x \in \mathcal{X}} F_{X}(x) = 1$\\
   Note:\\
    - $F_X(x) = \int_{-\infty}^x f_X(x) \diff x$\\
    - $\Pr(x_1 < X \le x_2) = F_X(x_2) - F_X(x_1)$

  \section{Supervised Learning}

  \subsection{Definitions}

  \texttt{Supervised Learning} is when correct answers are given to a training model and it uses them to make future perdictions following similar patterns\newline

  \texttt{Columns} are called  input variables or features or attributes\newline

  \texttt{Labels} or output variables are the outcome we are trying to perdict\newline

  \texttt{Training Example} or instance is a row in a table\newline

  \texttt{Data Set} is the whole table\newline

  $h$ is called a predictive model or \texttt{hypothesis}\newline

  \texttt{Classification} Is this A, B or C? (Binary if only 2 categories)\newline
  \texttt{Regression} How much or how many?\newline
  \texttt{Clustering} How is this organized?\newline
  \texttt{Reinforcement} What should i do next?\newline

  \subsection{Example Problem} 
  Given a data set $D \subset ({{\cal X}}\times {{\cal Y}})^n$, find a function: $$h : {{\cal X}}\rightarrow {{\cal Y}}$$ 
  such that $h({\bf x})$ is a good predictor for the value of $y$.

  \subsection{Steps to solve a supervised learning problem}
  1.  Decide what the input-output pairs are.\\
  2.  Decide how to encode inputs and outputs.\\
      This defines the input space ${{\cal X}}$, and the output space
      ${{\cal Y}}$.\\
  3.  Choose model space/hypothesis class ${{\cal H}}$.\\
  4.  Choose an error function (cost function) to define the best
      model in the class\\
  5.  Choose an algorithm for searching efficiently through the space
      of models to find the best.\newline

  \subsection{Linear Hypothesis}
  Suppose $y$ was a linear function of ${\bf x}$:
    $$h_{\bf w}({\bf x}) = w_0 + w_1 x_1 + w_2 x_2 + \cdots$$

    $w_i$ are called  parameters or weights. Typically include an attribute $x_0=1$ (also called  bias
    term or  intercept term) 

  \subsection{Choosing Weights}

  \subsection{Error Minimization}
  Define an error function or a cost function to measure how much our prediction differs from the 'true' answer on the training data.\\
  The weights should make the prediction as close to the true values as possible.

  \subsection{Picking the error function}

  \subsection{Least Means Squares}
  Draw a linear line such that the sum of the distance between each the line and each training point (error) is as small as possible.

  \subsection{Linear Regression Summary}
  The optimal solution (minimizing sum-squared-error) can be computed
  in polynomial time in the size of the data set.\\
  A very rare case in which an analytical, exact solution is possible\newline

  \subsection{Improving Linear Regression}
   1.  Explicitly transform the data, i.e. create additional features\\
        -   Add cross-terms, higher-order terms\\
        -   More generally, apply a transformation of the inputs from
            ${\cal X}$ to some other space ${\cal X}'$, then do linear
            regression in the transformed space\\
    2.  Use a different model space/hypothesis class\newline

  \section{Performance Evaluation}

  \subsection{Performance of a Fixed Hypothesis}
   Define the loss (error) of the hypothesis on an example $(\x, y)$ as
		$$L(h(\x) ,y)$$

  Given a model $h$, (which could have come from anywhere), its
    generalization error is:
    $$E[L(h({{\bf X}}),Y)]$$

  Given a set of data points $(\x_i, y_i)$ that are realizations of $({{\bf X}},Y)$, we can compute the empirical error
    $$\bar\ell_{h,n} = \frac{1}{n}\sum_{i=1}^n L(h({{\x }}_i),y_i)$$

  \subsection{Sample Mean}

  Given a dataset, $\bar x_n$ is a fixed number. We use $\bar X_n$ to denote the random variable corresponding 
  to the sample mean computed from a randomly drawn dataset of size $n$.

  \subsection{Statistics, Parameters, and Estimation}
  A \texttt{statistic} is any summary of a dataset. (E.g. $\bar x_n$, sample median.)\\
  A statistic is the result of a \texttt{function} applied to a dataset.\newline

  A \texttt{parameter} is any summary of the distribution of a random variable. (E.g. $\mu_X$, median.)\\
  A parameter is the result of a \texttt{function} applied to a distribution.\newline

  \texttt{Estimation} uses a \texttt{statistic} (e.g. $\bar{x}_n$) to estimate a \texttt{parameter}
  (e.g. $\mu_X$) of the \texttt{distribution} of a \texttt{random variable}.\\
    - \texttt{Estimate}: value obtained from a specific dataset\\
    - \texttt{Estimator}: function (e.g. sum, divide by n) used to compute the estimate\\
    - \texttt{Estimand}: parameter of interest\newline

  The distribution of an estimator is called a \texttt{sampling distribution}

  \subsection{Bias}

  The expected difference between estimator and parameter. For example,\\

$$ E[\bar{X}_n - \mu_X] $$

  If 0, estimator is unbiased.\\
  Sometimes, $\bar{x}_n > \mu_X$, sometimes $\bar{x}_n < \mu_X$, but the long run average of these differences will be zero.

  \subsection{Variance}

  The expected squared difference between estimator and its mean\\
  $$ E[(\bar{X}_n - E[\bar{X}_n])^2] $$
  -   Positive for all interesting estimators.\\
  -   For an unbiased estimator\\
  $$ E[(\bar{X}_n - \mu_X)^2] $$\\
  -   Sometimes, $\bar{x}_n > \mu_X$, sometimes $\bar{x}_n < \mu_X$, but the \texttt{squared differences} are all positive and do not cancel out.

  \subsection{Normal Distribution}

  Normal distribution is defined by two parameters: $\mu_X, \sigma^2_X$.\\
  For an estimator like $\bar{X}_n$, if we know $\mu_{\bar{X}_n}$ and $\sigma^2_{\bar{X}_n}$, then we can say a lot about how good it is.

  \subsection{Central Limit Theorem}

  The sampling distribution of $\bar X_n$ is approximately normal if $n$ is big enough.\\
  $$ \sigma_n^2 = \frac{\sigma^2}{\sqrt{n}} $$ 
  is called the \texttt{standard error} and $\sigma^2$ is the variance of $X$.

  \subsection{Confidence Interval}

  Typically, we specify confidence given by $1 - \alpha$\\
  Use the sampling distribution to get  
  an interval that traps the parameter (estimand) with probability $1 - \alpha$.

  \subsection{Test Sets}

  Training error underestimates generalization error. It is a \texttt{biased estimator}.\\
  Create a test set Possibly of size $n = (1.96)^2\frac{\sigma_L^2}{d^2}$ where $\sigma_L^2$ 
  is the variance of the loss (which has to be guessed or estimated from training) and $d$ is half-width of a 95\% confidence interval.

  \section{Model Selection}

  \subsection{Overfitting}
  
  Larger model spaces *always* lead to lower training error.\\
  Small training error, large generalization error is known as \texttt{overfitting}

  \subsection{Strategy 1: A Validation Set}
  A separate \texttt{validation set} can be used for model selection.\\
    - Train on the training set using each proposed model space\\
    - Evaluate each on the validation set, identify the one with lowest *validation* error\\
    - Choose the simplest model with performance < 1 std. error worse than the best.

  \subsection{Problems with Single-Partition Approach}
  Pros:\\
    -  Measures what we want: Performance of the actual learned model.\\
    -  Simple\\
  Cons:\\
    -  Smaller effective training sets make performance and performance estimates more variable.\\
    -  Small validation sets can give poor model selection\\
    -  Small test sets can give poor estimates of performance\\
    -  For a test set of size 100, with 60 correct classifications,
        95% C.I. for actual accuracy is $(0.497,0.698)$.\\

  \subsection{k-fold cross-validation}
  Divide the instances into $k$ disjoint partitions or folds of size $n/k$\\
  Loop through the partitions $i = 1 ... k$:\\
    -  Partition $i$ is for evaluation (i.e., estimating the performance of the algorithm after learning is done)\\
    -  The rest are used for training (i.e., choosing the specific model within the space)\\
  'Cross-Validation Error' is the average error on the evaluation partitions. Has lower variance than error on one partition.\newline 

  As with a single validation set, select most parsimonious model whose error is no more than one standard error above the error of the best model.

  \subsection{Summary for Model Selection}
   The training error decreases with the complexity (size) of the model space\\
   Generalization error decreases at first, then starts increasing\\
   Set aside a validation set helps us find a good model space\\
   We then can report unbiased error estimate, using a test set\\
   Cross-validation is a lower-variance but possibly biased version of this approach. It is standard.\\

   \section{Classification}
   
   \subsection{Linear Methods for Classification}
   Classification tasks\\
   Loss functions for classification\\
   Logistic Regression\\
   Support Vector Machines\\

   \subsection{Logistic Regression}
   Determines the probaability of a feature occuring based on some numeric features.\\
   Perdict the probability (0 to 1) of anything occuring\\
   Specify dependent and independent features\\
   Can classify groups of data using probability categories like all above 0.5 in one and below 0.5 in another

   \subsection{Decision Boundary}
   Just cut the data into parts using parts some summary of the data\\
   Cut off the data where Y is 1 and x is greater than 0.25\\
   Class = R if $\Pr(Y=1|X=x) > 0.25$\\
   
   Cut off the data where Y is 1 and x is greater than 0.5\\
   Class = R if $\Pr(Y=1|X=x) > 0.5$\\

   \subsection{Linear Support Vector Machines}
   Linear classifiers that focus on learning the decision
    boundary rather than the conditional distribution
    $P(Y=y|\mathbf{X}=\x)$

  \subsection{Perceptrons}
  If the data is linearly seperatable, the perceptron will find the solution given infinte iterations\\
  Blindingly Fast\\
  Solutions are non-unique\\

  \subsection{Perceptron Learning Rule}

    Initialize ${{\mathbf{w}}}$ and $w_0$ randomly\\
    While any training examples remain incorrecty classified\\
        Loop through all misclassified examples\\
        For misclassified example $i$, perform the updates:\\
        $${{\mathbf{w}}}\gets {{\mathbf{w}}}+ \delta y_i{{\x}}_i,~~~~~w_0\gets w_0 + \delta y_i$$
        where $\delta$ is a step-size parameter.

  \subsection{Support Vector Machines}

  A optimization criterion (the "margin") guarantees uniqueness and has theoretical advantages\\
  Natural handling nonseparable data by allowing mistakes\\
  An efficient way of operating in expanded  feature spaces: "kernel trick"\\
  SVMs can also be used for multiclass classification and regression.\\

  \subsection{Soft Margin Classifiers}
  
  Constraints are relaxed and misclassifiations are allowed.

  \subsection{Visualizations}
  
  \texttt{Scatterplot Matrix} shows the relation between all variables and is a good first step.\\
  \texttt{Correlation Plot/Matrix} shows how two variables are correlated or dependent on each other

  \section{Non-Linear Models}

  \subsection{Bias-Variance of K-NN (K Nearest Neighbors)}
  If $k$ is low, very non-linear functions can be approximated, but we also 
  capture the noise in the data Bias is low, variance is high\\

  If $k$ is high, the output is much smoother, less sensitive to data variation
  High bias, low variance\\

  A validation set can be used to pick the best $k$
  
  \subsection{Lazy and Eager Learning}
  \texttt{Lazy} wait for query before generalizing\\
    E.g. Nearest Neighbor

  \texttt{Eager} generalize before seeing query

    E.g. SVM, Linear regression

  \subsection{Pros and Cons of Lazy and Eager}
  Eager learners must create global approximation\\
  Lazy learners can create many local approximations\\
  An eager learner does the work off-line, summarizes lots of data with few parameters\\
  A lazy learner has to do lots of work sifting through the data at query time\\
  Typically lazy learners take longer time to answer queries and require more space\\

  \subsection{When to consider Non-parametric methods}
  When you have: instances that map to points in ${\mathbb R}^p$, 
not too many attributes per instance ($< 20$), lots of data\\

  - Advantages:\\

    -   Training is very fast\\
    -   Easy to learn complex functions over few variables\\
    -   Can give back confidence intervals in addition to the prediction\\
    -   Often wins if you have enough data\\

  - Disadvantages:\\

    -   Slow at query time\\
    -   Query answering complexity depends on the number of instances\\
    -   Easily fooled by irrelevant attributes\\
    -   'Inference' is not possible\\

  \subsection{Stop Overfitting in Decision Tree}

  1.  Stop growing the tree when further splitting the data does not
        yield a statistically significant improvement\\

  2.  Grow a full tree, then \texttt{prune} the tree, by
        eliminating nodes

  \subsection{Decision Tree Summary}
  -   Very fast learning algorithms \\
-   Attributes may be discrete or continuous, no preprocessing needed\\
-   Provide a general representation of classification rules\\
-   Easy to understand! Though\\
    -   Exact tree output may be sensitive to small changes in data\\
    -   With many features, tests may not be meaningful\\
-   In standard form, good for (nonlinear) piecewise axis-orthogonal
    decision boundaries â€“ not good with smooth, curvilinear boundaries\\
-   In regression, the function obtained is discontinuous, which may not
    be desirable\\
-   Good accuracy in practice many applications\\




\end{document}
