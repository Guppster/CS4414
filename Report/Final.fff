\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/languages.png}}
\caption{Frequency of each programming language that occurs in the dataset of
solutions } \label{languages} \end{center} \vskip -0.2in \end{figure}

\citet{kernalnlp}

\subsection{Feature Selection}\label{feature-selection}

\subsubsection{Question Data}\label{question-data}

The most useful features in the question data files are question ID,
title, difficulty level, question statement, and time limit. This data
will need to be cleaned by removing some common text that exists in
every question. For example: ``All submissions for this problem are
available''. Then, this data can be inner joined to the solutions
dataset with the key `Question ID' to determine insights about how the
questions effect code style.

\subsubsection{Solution Data}\label{solution-data}

The most useful features in the solution data files are solution ID,
status, time taken, memory taken, and language. First this data will be
filtered to contain only the C family of languages. The C family is any
language that starts with a C followed by a space. Then solution ID will
be used to outer merge data to the relevant code data provided in
separate files so we have more information about each individual
solution.

\subsubsection{Code Data}\label{code-data}

The code data files only contain two columns, the first one containing
the solution ID and the second containing the code string. The solution
ID will be used to merge the data with the solution data file and that
data frame will act as the base for the rest of the analysis on code.

In order to make the connection between code functionality and code
style, a large part of the experiment is obtaining an accurate and
unbiased metric for code quality. The decision to narrow the dataset to
only the C family shows it's importance here as we can use a single tool
to evaluate quality

The tool we will be using is \texttt{cpplint}. It is open source and
follows
\href{https://google.github.io/styleguide/cppguide.html}{Google's C
style guide}. We wrote a ruby script to run this tool on each row of the
merged solutions + code dataset and add another feature to each
instance. The feature is an integer representing the number of code
style errors the code had

\section{Analysis}\label{analysis}

\subsection{Experimental Methods}\label{experimental-methods}

The main relationship we are interested in is the effect of Quality on
the success rate. To begin to solve that problem we first look at the
success rate grouped by UserID. Figure \ref{uservsuccess} shows that
large chunks of users are placed at very low success and very high
success. This is what we would expect to see if code quality was
influencing results because users who write better code would be correct
more often. Other factors may be causing this behaviour so we continue
analyzing the data.

\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/UserVsSuccess.png}}
\caption{ Histogram showing the distribution of users and their success rates } \label{uservsuccess} \end{center} \vskip -0.2in \end{figure}

Next, we will see if you can predict the amount of style errors based on
weather a solution will be ``accepted'' or end in one of the many error
codes. This will be done by creating a model using linear regression so
we can do predict the Quality value based on the status feature. Before
creating the model the data was split into 2 parts for training and
testing. The training data frame was 75\% of the data and the testing
data frame was the remaining 25\%. Figure \ref{linearRegression} shows a
visual representation of the binary classification. It is clear that the
unaccepted answers had more code style errors in their submissions.

\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/linearregression.png}}
\caption{ Predicting Quality based on Success } \label{linearRegression} \end{center} \vskip -0.2in \end{figure}

After developing the model, the test set was used to determine the
models error. Root means squared error was used for error calculation so
there is extra weight given to large errors. The error value we received
for this model was 76. The dependent variable was `Number of Errors'
therefore this error is low because RMSE has the same units as the
dependent variable. The RMSE of the training data frame is 78 which
leads us to believe the model is not over-fitting. This shows that given
a correct or incorrect solution our model can guess how bad the code
style will be.

Next, we will use logistic regression to predict the chance that a
solution will be correct based on the Quality feature. After using
\texttt{glm} with Quality and Success features, the summary of the model
showed that Quality statistically significant with a low p-value. The
same 75\% training data 25\% testing data split was used in this model
as well. Using the testing set to predict and calculate error using MSE
with a 0.5 decision boundary, the error returned was 0. The model was
unsuccessful.

\section{Discussion \& Conclusion}\label{discussion-conclusion}

In conclusion, the analysis attempted in this study provide some
evidence that modern code quizzing websites and other ``just in time''
teaching resources can enhance their material by adding code quality
checks into their evaluation. Figures \ref{uservsuccess} \& 4 show how
the users of code evaluation websites have very dispersed skill levels
and code quality could be the key to bridging the gap.

\bibliography{bibliography.bib}
\bibliographystyle{plainnat}

\end{document}
