---
output: 
  pdf_document:
    citation_package: natbib
    toc: true
    number_sections: false
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: ./cs4414.tex
title: "Evaluating Solution Quality and Problem Difficulty Utilizing Code Metrics"
runhead: "Data Science Report"
authorname: Gurpreet Singh
studentnumber: 250674134
institute: The University Of Western Ontario
email: gsingh95@uwo.ca
date: "`r format(Sys.time(), '%B %d, %Y')`"
indent: yes
bibliography: bibliography.bib
bibstyle: plainnat
colorlinks: true
boldabstract: Discovering the effects of code style on code functionality and problem structure.
abstract: This study aims to showcase how well formatted, reusable and maintainable code is fundamentally better in all circumstances by looking at over 1 million results from a competitive programming website and analyzing the correlation between question and solution. The goal is to promote code quality checking in online 'just in time' teaching resources to improve the performance of interviewers, researchers, and students.
---

# Description of Applied Problem

## Problem identification

A large amount of educational material related to programming exists on the
internet but the majority of which is not well structured or presented. An
applied problem that can be observed from educational material found online is
that code quality is often left mutually exclusive from code functionality.
@justintimeteaching

This leads to some students believing it is acceptable to write code that
produces the correct result even if the process behind it is not correct.
Online code challenge websites like CodeChef.com do not take into account the
style and quality metrics of a code submission when judging competitions.
@codechef Cutting corners in the learning process advances into a complete
disregard for best practices in open source software and in the workplace which
results in a larger amount of errors. Readability of code is an essential
metric in software engineering and can be improved even with simple additions
of whitespace between lines. @codereadability

Computer code written by researchers and other individuals who are just trying
to accomplish a result in any way possible is often of the worse quality. This
is because they learn using the 'just in time' mentality and the resources
online that promote this mentality disregard code quality. @justintimeteaching
Lack of code quality directly reduces it's re-usability because other
programmers have a harder time understanding what the code is doing. If these
teaching resources could use questions and checks that promote better code
quality, many technical innovations could be made. Researchers would be more
educated on how to create reusable code and this would influence developers to
take their ideas and apply them to real life use cases. @reusability

Code quality post processing software is often used in production development
environments to ensure good style choices. These checks are much less useful at
this senior level than they would be at an educational level. If programming
style can be judged on a submission, companies conducting technical interviews
will be able to better judge applicants and make a more informed decision. 

This study will focus on proving that code quality can have an influence on
code functionality, as well as which kinds of questions influence good or bad
code styles.

## Approach Strategy

A solution to these problems is linking the scoring process in programming
problems to a metric derived from running code quality checks on the
submission.

Not only will this analysis benefit educational institutes but also companies
and competitions that judge people on their code submissions. 

# Data Description

## CodeChef Dataset

CodeChef.com is a competitive programming web application that has posted all
of their questions and solutions onto the data science website, Kaggle. The
data consists of a questions comma separated file, a solutions comma separated
file and 3 files that show the code associated with each solution id. The set
contains about 1000 problem statements and over 1 million code solutions
submitted. This should be a more than sufficient to make a training and test
data set. 

## Code Submission Language Density

The code submissions are written in many different programming languages and
each language has it's own code analysis tool. Therefore, to make the process
simpler and come up with higher quality results, the data will need to be
filtered by the top languages used. Figure~\ref{languages} shows that C++,
Java, C and Python are the most popular submissions in this dataset. The 
majority of the dataset is describing submissions in the C family of languages.
They are also the easiest to group together and process allowing for fair 
comparison of results, therefore, this study will focus only on the C family.
This reduces the dataset to just under 600000 rows of valid data.

\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/languages.png}}
\caption{Frequency of each programming language that occurs in the dataset of
solutions } \label{languages} \end{center} \vskip -0.2in \end{figure}
@kernalnlp

## Feature Selection

### Question Data

The most useful features in the question data files are question ID, title,
difficulty level, question statement, and time limit. This data will need to 
be cleaned by removing some common text that exists in every question. For 
example: "All submissions for this problem are available". Then, this data
can be inner joined to the solutions dataset with the key 'Question ID' to determine
insights about how the questions effect code style.

### Solution Data 

The most useful features in the solution data files are solution ID, status,
time taken, memory taken, and language. First this data will be filtered to
contain only the C family of languages. The C family is any language that
starts with a C followed by a space. Then solution ID will be used to outer
merge data to the relevant code data provided in separate files so we have more
information about each individual solution.

### Code Data

The code data files only contain two columns, the first one containing the
solution ID and the second containing the code string. The solution ID will be
used to merge the data with the solution data file and that data frame will act
as the base for the rest of the analysis on code.

In order to make the connection between code functionality and code style, a
large part of the experiment is obtaining an accurate and unbiased metric for
code quality. The decision to narrow the dataset to only the C family shows
it's importance here as we can use a single tool to evaluate quality

The tool we will be using is `cpplint`. It is open source and follows [Google's
C style guide](https://google.github.io/styleguide/cppguide.html). We wrote
a ruby script to run this tool on each row of the merged solutions + code
dataset and add another feature to each instance. The feature is an integer
representing the number of code style errors the code had

# Analysis

## Experimental Methods

The main relationship we are interested in is the effect of Quality on the
success rate. To begin to solve that problem we first look at the success rate
grouped by UserID. Figure \ref{uservsuccess} shows that large chunks of users
are placed at very low success and very high success. This is what we would
expect to see if code quality was influencing results because users who write
better code would be correct more often. Other factors may be causing this
behaviour so we continue analyzing the data.

\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/UserVsSuccess.png}}
\caption{ Histogram showing the distribution of users and their success rates } \label{uservsuccess} \end{center} \vskip -0.2in \end{figure}

Next, we will see if you can predict the amount of style errors based on
weather a solution will be "accepted" or end in one of the many error codes.
This will be done by creating a model using linear regression so we can do
predict the Quality value based on the status feature. Before creating the
model the data was split into 2 parts for training and testing. The training
data frame was 75% of the data and the testing data frame was the remaining
25%. Figure \ref{linearRegression} shows a visual representation of the binary
classification. It is clear that the unaccepted answers had more code style
errors in their submissions.

\begin{figure}[ht] \vskip 0.2in \begin{center}
\centerline{\includegraphics[width=\columnwidth]{../images/linearregression.png}}
\caption{ Predicting Quality based on Success } \label{linearRegression} \end{center} \vskip -0.2in \end{figure}

After developing the model, the test set was used to determine the models
error. Root means squared error was used for error calculation so there is
extra weight given to large errors. The error value we received for this model
was 76. The dependent variable was 'Number of Errors' therefore this error is
low because RMSE has the same units as the dependent variable. The RMSE of the
training data frame is 78 which leads us to believe the model is not
over-fitting. This shows that given a correct or incorrect solution our model
can guess how bad the code style will be.

Next, we will use logistic regression to predict the chance that a solution
will be correct based on the Quality feature. After using `glm` with Quality
and Success features, the summary of the model showed that Quality
statistically significant with a low p-value. The same 75% training data 25%
testing data split was used in this model as well. Using the testing set to
predict and calculate error using MSE with a 0.5 decision boundary, the error
returned was 0. The model was unsuccessful. 

# Discussion & Conclusion

In conclusion, the analysis attempted in this study provide some evidence that
modern code quizzing websites and other "just in time" teaching resources can
enhance their material by adding code quality checks into their evaluation.
Figures \ref{uservsuccess} & 4 show how the users of code evaluation websites
have very dispersed skill levels and code quality could be the key to bridging
the gap.
